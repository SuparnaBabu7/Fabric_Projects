# ğŸ›’ Jumia E-Commerce Sales - Microsoft Fabric Data Engineering Project

## ğŸš€ Overview

This end-to-end data engineering project was implemented using **Microsoft Fabric**, simulating a real-world E-Commerce sales pipeline using **Jumia sales data**. The project demonstrates advanced capabilities including data ingestion, transformation, orchestration, and semantic modelingâ€”all in a unified Fabric workspace.

---

## ğŸ§± Project Structure

![Project Architecture](ProjectArchetecture.png)

- **Workspace**: `Ecommerce-Project-WS`
- **Lakehouse**: `ECOMP-PROJ-LH`
  - Subfolders:
    - `/Current`
    - `/Archive`

---

## ğŸ—‚ï¸ Data Ingestion

- Uploaded the raw **Sales** and **Returns** Excel files into the `Current` folder.
- Created **Lakehouse tables** by reading these files using notebooks.

---

## ğŸ”§ Data Transformation (Bronze Layer)

- Created initial notebook `Bronze_Sales`:
  - Read and joined Sales and Returns data on `Order ID`.
  - Cleaned unnecessary columns.
  - Created a **temporary view** to transform into the Bronze table.
  - Merged records into the Bronze table using `SparkSQL`.

---

## ğŸ¥‡ Gold Layer Tables (Dimension & Fact)

Each gold table was created using dedicated notebooks with incremental loading:

### ğŸ§© Dimension Tables

- `Gold_Returns`
- `Gold_OrderPriority`
- `Gold_ShipMode`
- `Gold_Customer`
- `Gold_Product`
- `Gold_Date`

Special steps included:
- Using `monotonically_increasing_id()` for surrogate keys.
- Generating custom date ranges (`2015-01-01` to `2030-12-31`) for the date dimension.
- Schema enforcement and versioned loading.

### ğŸ“Š Fact Table

- `Gold_Fact_Sale` created using merged views and updated using `MERGE` statements.

---

## ğŸ“¦ Data Archival

- Implemented archival notebook:
  - Used mount paths and file system operations to **move ingested files from `/Current` to `/Archive`**.

---

## ğŸ§¬ Pipeline Orchestration

- Created a central `Run_Load` notebook to invoke all other notebooks sequentially.
- Built a **Microsoft Fabric Data Pipeline** to execute this notebook end-to-end.
- âœ… **Pipeline executed successfully.**

---

## ğŸ“ˆ Semantic Model & Power BI

- Built a **Semantic Model** on top of Gold tables.
- Imported the model into **Power BI Desktop**.
- Created a fully interactive **Power BI Report** for sales analysis.
- Established relationships between Fact and Dimension tables.

---

## ğŸ“Š Key Highlights

| Feature                          | Count         |
|----------------------------------|---------------|
| âœ… Pipelines Created             | 1             |
| ğŸ““ Notebooks Used                | 10+           |
| ğŸ“š Semantic Models               | 1             |
| ğŸ“Š Power BI Reports              | 1             |

---

## ğŸ“ Resources

- ğŸ¥ [Video Guide](https://youtu.be/vK9tDFuhQ6Y?si=6LKzsbaegZ5M0cfe)
- ğŸ“‚ [Project Architecture Diagram](ProjectArchetecture.png)

---

ğŸ”— **Project done by**: ğŸ‘¨â€ğŸ’¼ [Inturi Suparna Babu](https://www.linkedin.com/in/inturi-suparna-babu-312b59270/)

## ğŸ™Œ Acknowledgements

This project is inspired by real-world data challenges in the e-commerce domain and leverages the full capabilities of **Microsoft Fabricâ€™s Lakehouse architecture** and **Power BI analytics**.

---

â­ *If you found this helpful, please consider giving the repo a star and following me on LinkedIn for more data engineering content!*


